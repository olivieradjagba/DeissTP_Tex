\section{Related work}
In our discussion of related work, we will focus our attention on alternative methods for finding optimal decision trees, that is, decision trees that achieve the best possible score under a given set of constraints.

%\citeauthoryear{gf}{1336}
Most attention has been given in recent years to MIP-based approaches. \cite{bertsimas:2017optimal} %Bertsimas and Dunn (2017) 
developed an approach for finding decision trees of a maximum depth K that optimize misclassification error. They use K to model the problem in a MIP model with a fixed number of variables; a MIP solver is then used to find the optimal tree.

\cite{verwer:2019learning} %Verwer and Zhang (2019)
proposed BinOCT, an optimization of this approach, focused on how to deal with numerical data. To deal with numerical data, decision trees need to identify thresholds that are used to separate examples from each other. A MIP model was proposed in which fewer variables are needed to find high-quality thresholds; consequently, it was shown to work better on numerical data.

A benefit of MIP-based approaches is that it is relatively easy from a modeling perspective to add linear constraints or additional linear optimization criteria. \cite{aghaei:2019learning} %Aghaei, Azizi, and Vayanos (2019)
exploit this to formalize a learning problem that also takes into account the fairness of a prediction.

\cite{verwer:2019learning} %Verhaeghe et al. (2019)
recently proposed a Constraint Programming (CP) approach to solve the same problem. It supports a maximum depth constraint and a minimum support constraint, but only works for binary classification tasks. It also relies on branch-and-bound search and caching, but uses a less efficient caching strategy. The approach in the present paper is easily implemented and understood without relying on CP systems.

Another class of methods for learning optimal decision trees is that based on SAT Solvers \cite{narodytska:2018verifying,bessiere:2009minimising}. SAT-based studies, however, focus on a different type of decision tree learning problem than the MIP-based approaches: finding a decision tree of limited size that performs 100\% accurate predictions on training data. These approaches solve this problem by creating a formula in conjunctive normal form, for which a satisfying assignment would represent a 100\% accurate decision tree. We believe there is a need for algorithms that minimize the error, and hence we focus on this setting.

Most related to this work is the work of \cite{nijssen:2007mining,nijssen:2010optimal} %Nijssen and Fromont (2007; 2010)
on DL8, which relies on a link between learning decision trees and itemset mining. Similarly to MIP-based approaches, DL8 allows to find optimal decision trees minimizing misclassification error. DL8 does not require a depth constraint; it does however assume the presence of a minimum support constraint, that is, a constraint on the minimum number of examples falling in each leaf. In the next section we will discuss this approach in more detail. This discussion will show that DL8 can easily be used in settings identical to those in which MIP and CP solvers have been used. Subsequently, we will propose a number of significant improvements, allowing the itemset-based approach to outperform MIP-based and CP-based approaches.

